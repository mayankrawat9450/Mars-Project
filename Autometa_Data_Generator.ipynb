{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOw4Ow8hBDLO",
        "outputId": "cde8303e-7e78-4d25-b5fb-37f3a566fa98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (6.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors pyngrok transformers scikit-learn pdfplumber python-docx pytesseract langdetect spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!apt-get install -y tesseract-ocr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your own ngrok auth token\n",
        "!ngrok config add-authtoken 2yrAm1yrFsl2KmvLeumqKoLxZaU_6HGgQnvaRfMHNv1X6gE7s\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ry61oDsBQKO",
        "outputId": "636bac87-c752-415f-e975-e0806dd11ce6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"templates\", exist_ok=True)\n",
        "os.makedirs(\"static\", exist_ok=True)\n",
        "\n",
        "# index.html\n",
        "with open(\"templates/index.html\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>üìÑ Metadata Generator</title>\n",
        "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <h1>üìÅ Upload a Document</h1>\n",
        "    <form method=\"POST\" enctype=\"multipart/form-data\">\n",
        "      <input type=\"file\" name=\"file\" required><br>\n",
        "      <button type=\"submit\">Generate Metadata</button>\n",
        "    </form>\n",
        "\n",
        "    {% if metadata %}\n",
        "    <div class=\"result\">\n",
        "      <h2>üìå Title</h2>\n",
        "      <p>{{ metadata.title }}</p>\n",
        "\n",
        "      <h2>üìä Word Count</h2>\n",
        "      <p>{{ metadata.word_count }}</p>\n",
        "\n",
        "      <h2>üåê Language</h2>\n",
        "      <p>{{ metadata.language }}</p>\n",
        "\n",
        "      <h2>üëÄ Preview</h2>\n",
        "      <pre>{{ metadata.preview }}</pre>\n",
        "\n",
        "      <h2>üîç Summary</h2>\n",
        "      <textarea readonly>{{ metadata.summary }}</textarea>\n",
        "\n",
        "      <h2>üìù Keywords</h2>\n",
        "      <ul>\n",
        "        {% for keyword in metadata.keywords %}\n",
        "          <li>{{ keyword }}</li>\n",
        "        {% endfor %}\n",
        "      </ul>\n",
        "\n",
        "      <h2>üè∑Ô∏è Named Entities</h2>\n",
        "      <ul>\n",
        "        {% for entity in metadata.named_entities %}\n",
        "          <li>{{ entity }}</li>\n",
        "        {% endfor %}\n",
        "      </ul>\n",
        "\n",
        "      <h2>üì• Download</h2>\n",
        "      <form method=\"POST\" action=\"/download\">\n",
        "        <input type=\"hidden\" name=\"data\" value='{{ metadata | tojson }}'>\n",
        "        <button type=\"submit\">‚¨áÔ∏è Download Metadata (JSON)</button>\n",
        "      </form>\n",
        "    </div>\n",
        "    {% endif %}\n",
        "  </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# style.css\n",
        "with open(\"static/style.css\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "body {\n",
        "  font-family: Arial, sans-serif;\n",
        "  background-color: #f1f1f1;\n",
        "  padding: 30px;\n",
        "}\n",
        ".container {\n",
        "  max-width: 700px;\n",
        "  margin: auto;\n",
        "  background-color: white;\n",
        "  padding: 30px;\n",
        "  border-radius: 10px;\n",
        "  box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
        "}\n",
        "textarea {\n",
        "  width: 100%;\n",
        "  height: 150px;\n",
        "  padding: 10px;\n",
        "  margin-top: 10px;\n",
        "  resize: vertical;\n",
        "}\n",
        "button {\n",
        "  background-color: #4CAF50;\n",
        "  color: white;\n",
        "  border: none;\n",
        "  padding: 10px 20px;\n",
        "  margin-top: 10px;\n",
        "  font-size: 16px;\n",
        "  cursor: pointer;\n",
        "}\n",
        ".result {\n",
        "  margin-top: 30px;\n",
        "}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "Mu3Pj0CQBZEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, render_template, send_file\n",
        "from pyngrok import ngrok\n",
        "import os, json\n",
        "import pytesseract, pdfplumber\n",
        "from docx import Document\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "\n",
        "# Load NLP models\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "os.makedirs('uploads', exist_ok=True)\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def extract_text(path):\n",
        "    try:\n",
        "        ext = os.path.splitext(path)[1].lower()\n",
        "        if ext == '.pdf':\n",
        "            with pdfplumber.open(path) as pdf:\n",
        "                return '\\n'.join(p.extract_text() for p in pdf.pages if p.extract_text())\n",
        "        elif ext == '.docx':\n",
        "            return '\\n'.join(p.text for p in Document(path).paragraphs)\n",
        "        elif ext == '.txt':\n",
        "            return open(path, 'r', encoding='utf-8').read()\n",
        "        elif ext in ['.jpg', '.jpeg', '.png']:\n",
        "            return pytesseract.image_to_string(path)\n",
        "        else:\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] extract_text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def generate_summary(text):\n",
        "    try:\n",
        "        input_text = text[:1000]\n",
        "        return summarizer(input_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] summary: {e}\")\n",
        "        return \"Summary could not be generated.\"\n",
        "\n",
        "def extract_keywords(text):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(stop_words='english', max_features=10)\n",
        "        vectorizer.fit_transform([text])\n",
        "        return vectorizer.get_feature_names_out().tolist()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] keywords: {e}\")\n",
        "        return []\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] langdetect: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        return list(set(ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] NER: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- Routes ---\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    metadata = None\n",
        "    if request.method == \"POST\":\n",
        "        file = request.files.get(\"file\")\n",
        "        if file:\n",
        "            try:\n",
        "                path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
        "                file.save(path)\n",
        "                print(f\"[INFO] File saved to {path}\")\n",
        "\n",
        "                text = extract_text(path)\n",
        "                print(f\"[INFO] Extracted text length: {len(text)}\")\n",
        "\n",
        "                if text.strip():\n",
        "                    metadata = {\n",
        "                        \"title\": file.filename,\n",
        "                        \"word_count\": len(text.split()),\n",
        "                        \"language\": detect_language(text),\n",
        "                        \"preview\": '\\n'.join(text.strip().splitlines()[:3]),\n",
        "                        \"summary\": generate_summary(text),\n",
        "                        \"keywords\": extract_keywords(text),\n",
        "                        \"named_entities\": extract_named_entities(text)\n",
        "                    }\n",
        "                    print(\"[INFO] Metadata generated successfully.\")\n",
        "                else:\n",
        "                    print(\"[WARN] Empty text extracted.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] During processing: {e}\")\n",
        "    return render_template(\"index.html\", metadata=metadata)\n",
        "\n",
        "@app.route(\"/download\", methods=[\"POST\"])\n",
        "def download_metadata():\n",
        "    try:\n",
        "        data = json.loads(request.form['data'])\n",
        "        with open(\"metadata.json\", \"w\") as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        return send_file(\"metadata.json\", as_attachment=True)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] download: {e}\")\n",
        "        return \"Failed to generate metadata file\", 500\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXOq-3wWBddb",
        "outputId": "10e4fdfa-13ae-4cea-a932-c95abd0d97fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(5000)\n",
        "print(\"üåê App is running at:\", public_url)\n",
        "app.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg07q92nBgGB",
        "outputId": "8f0f8db1-0922-4757-bb6b-7a2412b991bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê App is running at: NgrokTunnel: \"https://d505-34-16-172-32.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 03:56:46] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 03:56:47] \"GET /static/style.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 03:56:48] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] File saved to uploads/5-mb-example-file.pdf\n",
            "[INFO] Extracted text length: 2895623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 03:59:26] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] NER: [E088] Text of length 2895623 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
            "[INFO] Metadata generated successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 03:59:27] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:00:37] \"\u001b[31m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:00:37] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:00:38] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:01:20] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:01:21] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:02:23] \"\u001b[31m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:03:00] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:03:00] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] File saved to uploads/5-mb-example-file.pdf\n",
            "[INFO] Extracted text length: 2895623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:05:38] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] NER: [E088] Text of length 2895623 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
            "[INFO] Metadata generated successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:05:39] \"\u001b[36mGET /static/style.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Jun/2025 04:06:11] \"POST /download HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RvhA5oRMBxcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}